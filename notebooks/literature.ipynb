{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2194981f-034e-47c7-82cb-86dfc0482450",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d9e11-e44b-4160-8921-088991a4093f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<pre>\n",
    "Looking at benchmarks for a given task gives me a good idea \n",
    "<a href=\"https://paperswithcode.com/task/image-captioning\">https://paperswithcode.com/task/image-captioning</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df8a8b-3a9b-41a8-b363-1ff832dc1e5d",
   "metadata": {},
   "source": [
    "# Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9057d14-c1a6-4715-91c8-5c2f4b81e8a7",
   "metadata": {},
   "source": [
    "<pre>\n",
    "When researching a new topic the first thing that I look after in the papers is the year of publishing, the citation counts, and the publishing institution. \n",
    "This gives me confidence that the paper is proven meaningful and impactful to the comunity. \n",
    "\n",
    "These are some of the impactful papers that I found and went trough.\n",
    "\n",
    "<a href=\"https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html\">2024 c=2432 - Improved baselines with visual instruction tuning</a>\n",
    "<a href=\"https://arxiv.org/abs/2305.06500\">2022 c=6978 - InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a>\n",
    "<a href=\"https://arxiv.org/abs/2301.12597\">2023 c=5988 - BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a>\n",
    "<a href=\"https://arxiv.org/abs/2201.12086\">2022 c=5052 - BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2205.14100\">2022 c=656  - GIT: A Generative Image-to-text Transformer for Vision and Language</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d3af4-f1d7-49b2-82d9-bb75e09ac5d9",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8c087-6e41-48a0-abe6-2e6c4ee47bf1",
   "metadata": {},
   "source": [
    "<pre>\n",
    "LlaVA - Way too fat for my machine. Wont be able to expriment with it.    \n",
    "</pre>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
